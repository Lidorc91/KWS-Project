# -*- coding: utf-8 -*-
"""Just train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OcMgzs7urQv5ZhmSZvRWYCMY8X9tNGck

##**Libraries**

Installations
"""

# Commented out IPython magic to ensure Python compatibility.
# #Dont show output
# %%capture
# ! pip install tensorflow

"""Imports"""

import tensorflow as tf
import numpy as np
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import pickle
from google.colab import drive
import pandas as pd
from tensorflow.keras.optimizers import Adam
import os

drive.mount('/content/drive')

"""## **Load preprocceced and scaled train and validation data**"""

# Load train data
file_path_x_train = '/content/drive/My Drive/Colab_Notebooks/x_train_scaled.pkl'

with open(file_path_x_train, 'rb') as f:
    X_train, train_labels = pickle.load(f)
    Y_train = np.array(train_labels) # command label

# load validation data
file_path_val_scaled = '/content/drive/My Drive/Colab_Notebooks/x_val_scaled.pkl'

with open(file_path_val_scaled, 'rb') as f:
    x_val, y_val = pickle.load(f)

# load test data
file_path_test_scaled = '/content/drive/My Drive/Colab_Notebooks/test_scaled.pkl'

with open(file_path_test_scaled, 'rb') as f:
    x_test, y_test = pickle.load(f)

#load labels
file_path_labels = '/content/drive/My Drive/Colab_Notebooks/labels.pkl'

with open(file_path_labels, 'rb') as f:
    labels = pickle.load(f)

"""#*Model train*

##**Model definition**
"""

model = tf.keras.Sequential()

model.add(layers.Conv2D(32, (7, 7), activation='relu',  input_shape=(63, 128, 1)))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (5, 5), activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.2))  # Dropout

model.add(layers.Conv2D(128, (5, 5), activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.2))  # Dropout

model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Dropout(0.2))  # Dropout

model.add(layers.Flatten())

model.add(layers.Dense(len(np.unique(labels)), activation='softmax'))

initial_learning_rate = 0.001
optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)

model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# Define the EarlyStopping callback and model save
checkpoint_filepath = '/content/drive/My Drive/Colab_Notebooks/models/beny_checkpoint_model.keras'

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',  # or 'val_accuracy', depending on what you want to monitor
    patience=5,          # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity
)
mdlcheckpoint = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath, monitor="val_loss", save_best_only=True
)

"""##Run Model"""

#running on X train with validation
history = model.fit(X_train, Y_train, batch_size=384, epochs=60, verbose=1,
                    validation_data=(x_val, y_val),
                    callbacks=[early_stop, mdlcheckpoint])
history_dict = history.history
#Run on test
test_loss, test_acc = model.evaluate(x_test, y_test)
print('Test accuracy:', test_acc)

"""Save model"""

path_model = '/content/drive/My Drive/Colab_Notebooks/models/model_beny_87.keras'
path_history = '/content/drive/My Drive/Colab_Notebooks/models/history_87.pkl'

with open(path_history, 'wb') as f:
    pickle.dump(history.history, f)
model.save(path_model)

#remove history variable from memory
import gc
history = None
gc.collect()

"""Load model"""

path_model = '/content/drive/My Drive/Colab_Notebooks/models/model_beny_87.keras'
path_history = '/content/drive/My Drive/Colab_Notebooks/models/history_87.pkl'

model = tf.keras.models.load_model(path_model)
with open(path_history, 'rb') as f:
    history_dict = pickle.load(f)

"""Summary graph"""

Losses = pd.DataFrame(history_dict)
Losses.plot()

"""##Run on test"""

test_loss, test_acc = model.evaluate(x_test, y_test)
print('Test accuracy:', test_acc)
